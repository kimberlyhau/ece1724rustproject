## components of inference streaming
- candle inference engine: loads the model and generates tokens 1 at a time
- Axum server (HTTP server): exposes a URL the client calls, and connects the client and inference engine
- Client (CLI or web): sends a prompt and displays tokens as they stream back.

## flow
- Client → Server
  - Client sends a POST request to `/generate` with a JSON body that includes the prompt
  - This is same as “POST with data” part of the web client lab
- Server → Engine
  - The Axum handler parses the JSON, then calls the inference engine code to start the generation loop
- Engine → Server
  - As llm-server produces tokens, it sends them one at a time (e.g., over a channel/stream)
  - The server reads each token as it appears
- Server → Client (streaming)
  - The server pushes each token back immediately using Server-Sent Events (SSE) or a similar streaming response
  - The client renders tokens as they arrive until done

## communication pattern
- Request (client → server)
  - POST `/generate`
  - JSON:
    ```json
    {
      "prompt": "what is a large language model?",
      "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
      "params": { "max_tokens?": 128 }
    }
    ```

- Streamed events (server → client, via SSE)
  - Per token (one per event):
    ```text
    data: {"token":"...", "index": 12}
    ```
  - Done:
    ```text
    data: {"done": true, "total_tokens": 123}
    ```
  - Error:
    ```text
    data: {"error": "message"}
    ```

## Axum (server side) notes
- Exposes the `/generate` endpoint (URL)
- Keeps a single `InferenceEngine` (model loaded once) in shared state.
- In the handler:
  - Read JSON from the POST body (like the lab’s `--json`).
  - Call `engine.generate(prompt, params)` which returns a stream of tokens.
  - Wrap tokens into SSE events and send them as they’re produced.
  - If the client disconnects, stop streaming and let the engine task end.

## client notes
- CLI/(curl for testing)
  - curl for testing:
    ```bash
    curl -N -X POST http://localhost:3000/generate --json '{"prompt":"..."}'
    ```
  - We used reqwest in lab to perform HTTP requests and implment a curl-like client
  - Now use reqwest to make a better user interface for our chatbot streaming
    - better CLI args (instead of passing in a json), streaming output, better design/formatting
  - `-N` means no buffering on client side. This will allow you to see tokens as they streamed in using server side events
- Web page?
  - the webpage would make HTTP requests directly from browser (fetch? EventSource? WebSocket?) to interact with Axum server
